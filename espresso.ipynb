{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shlex\n",
    "import logging\n",
    "import shutil\n",
    "import re\n",
    "from base64 import b64decode, b64encode\n",
    "import json\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from lxml.etree import tostring\n",
    "from lxml.html import soupparser\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command):\n",
    "    log.debug(f\"Command: {command}\")\n",
    "    command = shlex.split(command)\n",
    "    result = subprocess.run(command, capture_output=True, check=True)\n",
    "    if result.stdout:\n",
    "        log.debug(f\"Command stdout: {result.stdout.decode('utf-8')}\")\n",
    "    if result.stderr:\n",
    "        log.debug(f\"Command stderr: {result.stderr.decode('utf-8')}\")\n",
    "    return result\n",
    "\n",
    "# TODO: need better approach than changing dirs like this in case of failure? Maybe not an issue with scripts...\n",
    "def generate_help_files(work_dir, def_files, versions, database_dir):\n",
    "    # Minimal files needed for helpdoc to work\n",
    "    helpdoc_files = ['dev-tools/helpdoc', 'dev-tools/helpdoc.d', 'dev-tools/helpdoc.schema', 'dev-tools/input_xx.xsl', 'GUI/Guib/lib']\n",
    "\n",
    "    # Create work directory and go there\n",
    "    root = os.getcwd()\n",
    "    work_dir = os.path.join(root, work_dir)\n",
    "    # TODO: this is temporary\n",
    "    if os.path.exists(work_dir):\n",
    "        shutil.rmtree(work_dir)\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    os.chdir(work_dir)\n",
    "\n",
    "    # Commands to set up minimal helpdoc environment\n",
    "    qe_dir = os.path.join(work_dir, 'q-e')\n",
    "    cmd_clone = \"git clone --filter=blob:none --sparse https://gitlab.com/QEF/q-e.git\"\n",
    "    run_command(cmd_clone)\n",
    "    os.chdir(qe_dir)\n",
    "    cmd_fetch_tags = \"git fetch --all --tags\"\n",
    "    run_command(cmd_fetch_tags)\n",
    "\n",
    "    cmd_checkout_files = [\"git sparse-checkout add --skip-checks\"]\n",
    "    cmd_checkout_files = \" \".join(cmd_checkout_files + helpdoc_files + def_files)\n",
    "    run_command(cmd_checkout_files)\n",
    "\n",
    "    # Commands for picking the right versions\n",
    "    devtools_dir = os.path.join(qe_dir, 'dev-tools')\n",
    "    if not isinstance(versions, list):\n",
    "        versions = [versions]\n",
    "    for v in versions:\n",
    "        tag = v\n",
    "        tag += \"MaX\" if v in (\"6.3\", \"6.5\") else \"\"\n",
    "        tag += \"MaX-Release\" if v == \"6.7\" else \"\"\n",
    "        cmd_checkout_tag = f\"git checkout tags/qe-{tag} -b qe-{tag} --force\"\n",
    "        run_command(cmd_checkout_tag)\n",
    "        database_dir = os.path.join(root, database_dir, 'qe-'+v)\n",
    "        if not os.path.exists(database_dir):\n",
    "            os.makedirs(database_dir)\n",
    "\n",
    "        files = [os.path.join(qe_dir, def_file) for def_file in def_files]\n",
    "        for def_file in files:\n",
    "            dir = os.path.dirname(def_file)\n",
    "            cmd_link_xsl = f\"ln -sf {devtools_dir}/input_xx.xsl {dir}/input_xx.xsl\"\n",
    "            run_command(cmd_link_xsl)\n",
    "            cmd_helpdoc = f\"{devtools_dir}/helpdoc --version {v} {def_file}\"\n",
    "            run_command(cmd_helpdoc)\n",
    "\n",
    "            # Copy the generated files to the database directory using os module\n",
    "            xml_file = os.path.splitext(def_file)[0] + '.xml'\n",
    "            html_file = os.path.splitext(def_file)[0] + '.html'\n",
    "            # Explicit destination is needed to overwrite existing files\n",
    "            shutil.move(html_file, os.path.join(database_dir, os.path.basename(html_file)))\n",
    "            shutil.move(xml_file, os.path.join(database_dir, os.path.basename(xml_file)))\n",
    "\n",
    "    os.chdir(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO\n",
    "\n",
    "def parse_vargroup(vg, parent):\n",
    "    vars = []\n",
    "    type = vg.attrib['type']\n",
    "    info = vg.find('info')\n",
    "    if info is not None:\n",
    "        info = info.text\n",
    "    \n",
    "    for v in vg.findall('var'):\n",
    "        v_dict = {\n",
    "            'name': v.attrib['name'],\n",
    "            'parent': parent,\n",
    "            'type': type,\n",
    "            'info': info,\n",
    "            'dimension': 1,\n",
    "            'default': ' ',\n",
    "            'options': {},\n",
    "        }\n",
    "        vars.append(v_dict)\n",
    "\n",
    "    return vars\n",
    "\n",
    "def parse_var(v, parent):\n",
    "    opts = v.find('options')\n",
    "\n",
    "    # Deal with Info\n",
    "    info = None\n",
    "    if opts is not None:\n",
    "        info = opts.find('info')\n",
    "    else:\n",
    "        info = v.find('info')\n",
    "    if info is not None:\n",
    "        info = info.text\n",
    "    else:\n",
    "        info = ''\n",
    "    \n",
    "    options = {}\n",
    "    if opts is not None:\n",
    "        for o in opts.findall('opt'):\n",
    "            options.update({o.attrib['val']: o.text})\n",
    "    \n",
    "    default = v.find('default')\n",
    "    if default is not None:\n",
    "        default = default.text\n",
    "    else:\n",
    "        default = ''\n",
    "\n",
    "    v_dict = {\n",
    "        'name': v.attrib['name'],\n",
    "        'parent': parent,\n",
    "        'type': v.attrib.get('type', \"UNKNOWN\"),\n",
    "        'dimension': v.attrib.get('end', 1),\n",
    "        'info': info,\n",
    "        'default': default,\n",
    "        'options': options,\n",
    "    }\n",
    "\n",
    "\n",
    "    return v_dict\n",
    "\n",
    "def parse_group(g, parent):\n",
    "    vars = []\n",
    "    for e in g:\n",
    "        if e.tag in ('var', 'multidimension', 'dimension'):\n",
    "            vars.append(parse_var(e, parent))\n",
    "        elif e.tag == 'vargroup':\n",
    "            vars.extend(parse_vargroup(e, parent))\n",
    "        elif e.tag == 'group':\n",
    "            vars.extend(parse_group(e, parent)) \n",
    "    return vars\n",
    "\n",
    "type_map = {\n",
    "    \"character\": \"str\",\n",
    "    \"real\": \"float\",\n",
    "    \"integer\": \"int\",\n",
    "    \"logical\": \"bool\",\n",
    "    \"unknown\": \"unknown\"\n",
    "}\n",
    "\n",
    "def tidy_dict(d):\n",
    "    tidy_d = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(k, str):\n",
    "            k = tidy_str(k)\n",
    "        if v and isinstance(v, str):\n",
    "            v = tidy_str(v)\n",
    "        tidy_d[k] = v\n",
    "    return tidy_d\n",
    "\n",
    "def tidy_str(s):\n",
    "    s = s.replace('\\n', ' ').strip()\n",
    "    if s and s[0] == \"'\" and s[-1] == \"'\":\n",
    "        s = s[1:-1]\n",
    "    return s\n",
    "\n",
    "def tidy_vars(vars):\n",
    "    clean_vars = []\n",
    "    for v in vars:\n",
    "        name = v[\"name\"]\n",
    "        parent = v[\"parent\"].lower()\n",
    "        type = type_map[v[\"type\"].lower()]\n",
    "        dimension = v[\"dimension\"]\n",
    "        options = tidy_dict(v[\"options\"])\n",
    "        default = tidy_str(v[\"default\"])\n",
    "        info = tidy_str(v[\"info\"])\n",
    "\n",
    "        # Special cases\n",
    "        if name == 'A':\n",
    "            info = 'a in ANGSTROM'\n",
    "        elif name == 'B':\n",
    "            info = 'b in ANGSTROM'\n",
    "        elif name == 'C':\n",
    "            info = 'c in ANGSTROM'\n",
    "        elif name == 'cosAB':\n",
    "            info = 'cos angle between a and b (gamma)'\n",
    "        elif name == 'cosAB':\n",
    "            info = 'cos angle  between a and c (beta)'\n",
    "        elif name == 'cosBC':\n",
    "            info = 'cos angle between b and c (alpha)'\n",
    "        elif name == 'ibrav':\n",
    "            info = 'Bravais lattice choice'\n",
    "            options = {\n",
    "                0: \"Lattice in CELL_PARAMETERS\",\n",
    "                1: \"Cubic P (sc) lattice\",\n",
    "                2: \"Cubic F (fcc) lattice\",\n",
    "                3: \"Cubic I (bcc) lattice\",\n",
    "                -3: \"Cubic I (bcc) lattice\",\n",
    "                4: \"Hexagonal and Trigonal P lattice\",\n",
    "                5: \"Trigonal Rhombohedral lattice, 3-fold axis c\",\n",
    "                -5: \"Trigonal Rhombohedral lattice, 3-fold axis <111>\",\n",
    "                6: \"Tetragonal P (st) lattice\",\n",
    "                7: \"Tetragonal I (bct) lattice\",\n",
    "                8: \"Orthorhombic P lattice\",\n",
    "                9: \"Orthorhombic base-centered(bco) lattice\",\n",
    "                -9: \"Orthorhombic base-centered(bco) lattice\",\n",
    "                91: \"Orthorhombic one-face base-centered A-type lattice\",\n",
    "                10: \"Orthorhombic face-centered lattice\",\n",
    "                11: \"Orthorhombic body-centered lattice\",\n",
    "                12: \"Monoclinic P, unique axis c lattice\",\n",
    "                -12: \"Monoclinic P, unique axis b lattice\",\n",
    "                13: \"Monoclinic base-centered lattice\",\n",
    "                -13: \"Monoclinic base-centered lattice\",\n",
    "                14: \"Triclinic lattice\",\n",
    "            }\n",
    "        if type == \"bool\" and options == {}:\n",
    "            options = {\n",
    "                True: \"\",\n",
    "                False: \"\",\n",
    "            }\n",
    "        \n",
    "        clean_vars.append({\n",
    "            \"name\": name,\n",
    "            \"parent\": parent,\n",
    "            \"type\": type,\n",
    "            \"dimension\": dimension,\n",
    "            \"options\": options,\n",
    "            \"default\": default,\n",
    "            \"info\": info,\n",
    "        })\n",
    "    \n",
    "    return clean_vars\n",
    "\n",
    "def extract_vars(xml_filename):\n",
    "    pattern = re.compile(r'<a href=\"(.*?)\">\\s*(.*?)\\s*</a>')\n",
    "    with open(xml_filename, 'r') as f:\n",
    "        xmltext = f.read()\n",
    "        xmltext = xmltext.replace(\"<ref>\", \"\")\n",
    "        xmltext = xmltext.replace(\"</ref>\", \"\")\n",
    "        xmltext = xmltext.replace(\"<b>\", \"\")\n",
    "        xmltext = xmltext.replace(\"</b>\", \"\")\n",
    "        xmltext = pattern.sub(r'\\2 (\\1)', xmltext)\n",
    "        root = ET.parse(StringIO(xmltext)).getroot()\n",
    "\n",
    "    vars = []\n",
    "    #cards = []\n",
    "    for child in root:\n",
    "        if child.tag == 'namelist':\n",
    "            namelist_name = child.attrib['name']\n",
    "            for e in child:\n",
    "                if e.tag in ('var', 'multidimension', 'dimension'):\n",
    "                    vars.append(parse_var(e, namelist_name))\n",
    "                elif e.tag == 'vargroup':\n",
    "                    vars.extend(parse_vargroup(e, namelist_name))\n",
    "                elif e.tag == 'group':\n",
    "                    vars.extend(parse_group(e, namelist_name))\n",
    "        #elif child.tag == 'card':\n",
    "        #    cards.append(child)\n",
    "        \n",
    "    vars = tidy_vars(vars)\n",
    "\n",
    "    return vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a map from name -> {idm, html}\n",
    "def gen_idm_map(soup):\n",
    "    idm_map = {}\n",
    "    # Find all links with href = \"#idm*\", their text is the name\n",
    "    links = soup.xpath('//a[starts-with(@href, \"#idm\")]')\n",
    "    for a in links:\n",
    "        # The split accounts for some array edge cases in old documentation\n",
    "        name = a.text.split('(')[0] \n",
    "        if name.startswith('&'):\n",
    "            name = name[1:]\n",
    "        idm = a.attrib['href'][1:]\n",
    "        idm_map.update({name: {\"idm\": idm, \"html\": \"\"}})\n",
    "    # Find all a tags with name=\"name\", the table is an ancestor\n",
    "    for name, idm_dict in idm_map.items():\n",
    "        tags = soup.xpath(f'//a[@name=\"{name}\"]')\n",
    "        for a in tags:\n",
    "            html = None\n",
    "            # This accounts for most cases\n",
    "            for sibling in a.itersiblings():\n",
    "                if sibling.tag == 'table':\n",
    "                    html = sibling\n",
    "                    break\n",
    "            # This accounts for stuff in groups\n",
    "            if html is None:\n",
    "                for parent in a.iterancestors():\n",
    "                    if parent.tag == 'table':\n",
    "                        html = parent\n",
    "                        break\n",
    "            idm_dict[\"html\"] = html\n",
    "\n",
    "    return idm_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wipe_style(html):\n",
    "    to_delete = [\"style\", \"align\", \"valign\", \"width\"]\n",
    "    for attr in to_delete:\n",
    "        if attr in html.attrib:\n",
    "            del html.attrib[attr]\n",
    "\n",
    "\n",
    "def generate_webpage(html):\n",
    "    if isinstance(html, str):\n",
    "        print(html)\n",
    "    if html.tag == \"table\":\n",
    "        html.classes.add(\"tag-table\")\n",
    "        wipe_style(html)\n",
    "        tags = html.xpath(f\"//th\")\n",
    "        for tag in tags:\n",
    "            tag.classes.add(\"header-cell\")\n",
    "            wipe_style(tag)\n",
    "        tags = html.xpath(f\"//td\")\n",
    "        for child in tags:\n",
    "            if \"style\" in child.attrib:\n",
    "                style = child.attrib[\"style\"].split(\";\")\n",
    "                style = [s for s in style if s.strip()]\n",
    "                style = dict(s.split(\":\") for s in style)\n",
    "                style = {k.strip(): v.strip() for k, v in style.items()}\n",
    "                if \"background\" in style and \"text-align\" in style:\n",
    "                    bgcol = style[\"background\"]\n",
    "                    align = style[\"text-align\"]\n",
    "                    if bgcol == \"#ffffc3\" and align == \"left\":\n",
    "                        child.classes.add(\"type-cell\")\n",
    "                    elif bgcol == \"#ffffc3\" and align == \"right\":\n",
    "                        child.classes.add(\"datalabel-cell\")\n",
    "                    elif bgcol == \"#fff3d9\" and align == \"left\":\n",
    "                        child.classes.add(\"data-cell\")\n",
    "            elif \"colspan\" in child.attrib and child.attrib[\"colspan\"] == \"2\":\n",
    "                child.classes.add(\"description-cell\")\n",
    "            wipe_style(child)\n",
    "        tags = html.xpath(f\"//pre\")\n",
    "        for child in tags:\n",
    "            # child.string = tidy_str(child.text)\n",
    "            # print('found pre')\n",
    "            # Find if it has a <a> tag with href = \"#*\", if so, replace with <tt>\n",
    "            child.tag = \"p\"\n",
    "            links = child.xpath('//a[starts-with(@href, \"#\")]')\n",
    "            # Wrap the text in <tt> tags before and </tt> after\n",
    "            for a in links:\n",
    "                a.classes.add(\"tag-link\")\n",
    "                # TODO: change where href points to\n",
    "                # Should be like database_dir + webpage_dir + a.attribs['href'][1:]\n",
    "                a.attrib['href'] = a.attrib['href'][1:] + '.html'\n",
    "\n",
    "            # TODO: for ibrav, create a clean string and\n",
    "            # use that instead of the cleaning preocedure \n",
    "            # You should hash the string in the documentation \n",
    "            # And if it doesn't match use the unforomatted one from documentation\n",
    "            wipe_style(child)\n",
    "            string = tostring(child, encoding=\"unicode\")\n",
    "            string = tidy_str(string)\n",
    "            string = re.sub(r\"\\.TRUE\\.\", r\"<tt>.TRUE.</tt>\", string)\n",
    "            string = re.sub(r\"\\.FALSE\\.\", r\"<tt>.FALSE.</tt>\", string)\n",
    "            new_element = soupparser.fromstring(string)\n",
    "            child.getparent().replace(child, new_element)\n",
    "\n",
    "        webpage_template = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <link rel=\"stylesheet\" type=\"text/css\" href=\"../qe-tag.css\">\n",
    "        </head>\n",
    "        <body>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        webpage = soupparser.fromstring(webpage_template)\n",
    "        body = webpage.xpath(\"//body\")[0]\n",
    "        body.append(html)\n",
    "\n",
    "\n",
    "        return tostring(webpage, encoding=\"unicode\", pretty_print=True)\n",
    "\n",
    "\n",
    "def add_html_info(vars, html_filename):\n",
    "    with open(html_filename, \"r\") as f:\n",
    "        soup = soupparser.fromstring(f.read())\n",
    "\n",
    "    idm_map = gen_idm_map(soup)\n",
    "    #link_base = \"file:///\" + os.path.join(os.getcwd(), html_filename)\n",
    "    for v in vars:\n",
    "        name = v[\"name\"]\n",
    "        if name in idm_map:\n",
    "            v[\"idm\"] = idm_map[name][\"idm\"]\n",
    "            if idm_map[name][\"html\"] is not None:\n",
    "                # Screws up formatting for ibrav\n",
    "                webpage = generate_webpage(idm_map[name][\"html\"])\n",
    "                v[\"html\"] = b64encode(webpage.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "            else:\n",
    "                v[\"html\"] = \"\"\n",
    "            #v[\"link\"] = link_base + \"#\" + idm_map[name][\"idm\"]\n",
    "        else:\n",
    "            print(f\"WARNING: No HTML info for {name}\")\n",
    "    return vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_database(version, database_dir):\n",
    "    if isinstance(version, str):\n",
    "        version = [version]\n",
    "    for ver in version:\n",
    "        xml_filename = os.path.join(database_dir, 'qe-'+ver, \"INPUT_PW.xml\")\n",
    "        vars = extract_vars(xml_filename)\n",
    "        html_filename = os.path.join(database_dir, 'qe-'+ver, \"INPUT_PW.html\")\n",
    "        vars = add_html_info(vars, html_filename)\n",
    "        parents = list(set([v[\"parent\"] for v in vars]))\n",
    "        vars_dict = {p: {} for p in parents}\n",
    "        for v in vars:\n",
    "            vars_dict[v[\"parent\"]].update({v[\"name\"]: v})\n",
    "        json_filename = os.path.join(database_dir, 'qe-'+ver, \"database.json\")\n",
    "        with open(json_filename, \"w\") as f:\n",
    "            json.dump(vars_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No HTML info for Hubbard_J(i,ityp)\n",
      "WARNING: No HTML info for starting_ns_eigenvalue(m,ispin,I)\n"
     ]
    }
   ],
   "source": [
    "work_dir = 'work'\n",
    "def_files = ['PW/Doc/INPUT_PW.def', 'PP/Doc/INPUT_PROJWFC.def']\n",
    "versions = ['6.3', '6.8', '7.0', '7.2']\n",
    "database_dir = 'database/'\n",
    "gen_docs = False\n",
    "\n",
    "if gen_docs:\n",
    "    generate_help_files(work_dir, def_files, versions, database_dir)\n",
    "generate_database(versions, database_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dft-tutor-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
